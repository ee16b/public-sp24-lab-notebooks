{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Lab 8: SVD/PCA Classification for Voice Commands\n",
    "\n",
    "### EECS 16B: Designing Information Devices and Systems II, Spring 2024\n",
    "\n",
    "Written by Nathaniel Mailoa and Emily Naviasky (2016). \n",
    "\n",
    "Updated by Julian Chan (2018), Peter Schafhalter (2019). Vin Ramamurti and Zain Zaidi (Fall 2019)\n",
    "\n",
    "Updated by Kaitlyn Chan, Steven Lu (2021)\n",
    "\n",
    "Updated by Steven Lu, Megan Zeng, Ke Wang (2022)\n",
    "\n",
    "Updated by Shrey Aeron, Mingyang Wang, Megan Zeng (2022)\n",
    "\n",
    "Updated by Jessica Fan (2023)\n",
    "\n",
    "Updated by Junha Kim, Ryan Ma, Venkata Alapati (2023, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Introduction / Lab Note](#intro)\n",
    "\n",
    "### Project Part A\n",
    "\n",
    "* [Part 0: Preparing your Arduino](#part0)\n",
    "* [Part 1: Setting up your Circuit](#part1)\n",
    "* [Part 2: Data Collection](#part2)\n",
    "* [Part 3: Data Preprocessing](#part3)\n",
    "\n",
    "### Project Part B\n",
    "\n",
    "* [Part 4: PCA via SVD](#part4)\n",
    "* [Part 5: Clustering Data Points](#part5)\n",
    "* [Part 6: Testing your Classifier](#part6)\n",
    "* [Part 7: Arduino Implementation of PCA Classify](#part7)\n",
    "* [Appendix: Formatting Vectors for Arduino](#appendix)\n",
    "\n",
    "### Other\n",
    "* [Part 8: Final Lab Report Questions](#part8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## <span style=\"color:navy\">Introduction</span>\n",
    "\n",
    "S1XT33N is an obedient little robot that will follow the directions that you tell it. In Lab 7, you implemented the controller to enable S1XT33N to drive straight and turn. Now, we will implement voice control, and then put it all together in the last lab for a complete voice-controlled car!\n",
    "\n",
    "Unfortunately, S1XT33N does not understand our language, and some words, like \"left\" and \"right,\" sound very similar to S1XT33N (a strong single syllable), while other words are easier to distinguish. \n",
    "\n",
    "In this lab, we aim to find four command words that are easy for S1XT33N to tell apart (consider syllables and intonation). In order to do so, we will develop the SVD classifier that allows S1XT33N to tell the difference between the four commands, and then examine several different words and determine which ones will be easiest to classify by PCA.\n",
    "\n",
    "**Note that Lab 8 is intended to span two weeks. We split this lab into two phases as a recommendation of time allocation.**\n",
    "\n",
    "**Please read the <!-- LINK_LAB_NOTE_8-->[lab note](http://links.eecs16b.org/lab8-note)<!-- LINK_LAB_NOTE_8-->. It explains in detail what you will be doing in each part of the lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Lab\n",
    "\n",
    "Complete the pre-lab assignment before doing the lab. Submit your answers to the Gradescope assignment <!-- LINK_PRELAB_8-->[\"Pre-Lab 8\"](http://links.eecs16b.org/prelab8)<!-- LINK_PRELAB_8-->. Pre-Lab 8 is due on ** April 15, 11:59PM** \n",
    "\n",
    "## Help Request Form\n",
    "Use this [form](https://eecs16b.org/lab-help) to request help in your debugging / conceptual needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Classification Procedure\n",
    "Below is an overview of the classification procedure we will be following in this lab.\n",
    "1. Collect recordings of 6 different words. This will form our data set\n",
    "2. Preprocess the data to align the words\n",
    "2. Split the data into 2 sets: training and testing data\n",
    "3. Perform SVD and evaluate on the training data split\n",
    "4. Select the 4 best words to use as your commands\n",
    "5. Classify each word using clustering in the PCA basis\n",
    "6. Evaluate perfornace by running the model on testing data\n",
    "7. Make sure you (and your GSI) are satisfied with the classifier's accuracy.\n",
    "\n",
    "The goals of this phase are as follows:\n",
    "- Collect data and preprocess\n",
    "- Code and run PCA + Classifier on all 4 commands\n",
    "- Check accuracy\n",
    "\n",
    "### Side Note: Datasets in Machine Learning Applications\n",
    "It is common practice in machine learning applications to split a dataset into a training set and testing set (some common ratios for train:test are 80:20 or 70:30). In this lab, we will split 70:30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0'></a>\n",
    "## <span style=\"color:navy\">Part 0: Preparing your Arduino</span>\n",
    "\n",
    "For this entire lab, power your car with the stationary configuration below. The Arduino is powered through the USB while the front end circuit is powered through the power supply. Leave the motors unpowered because this lab does not require the motor circuits. <span style=\"color:red\">**Your batteries should be disconnected.**</span>\n",
    "\n",
    "<img width=\"600px\" src=\"images/PowerDiagram_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "## <span style=\"color:navy\">Part 1: Verifying your Circuit</span>\n",
    "\n",
    "### Materials\n",
    "- Mic board front-end circuit\n",
    "- Arduino + USB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Front End Verification\n",
    "\n",
    "1. Hook up your front end circuit\n",
    "2. Connect your circuit to the Arduino:\n",
    "    - **Pin `A2` to the microphone front end circuit output (output of non-inverting amplifier for low-pass filter).**\n",
    "    - GND pin to the ground rail of the breadboard\n",
    "3.  **Use the oscilloscope to probe the output of the microphone circuit (output of non-inverting amplifier for low-pass filter).** Make sure the waveform is centered at 2.5V\n",
    "    - Ensure there is a visible response when you talk/produce some other sound (but not railing). The signal should me _mostly_ flat when there is no sound\n",
    "    - Tune your micboard to match these parameters if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "## <span style=\"color:navy\">Part 2: Data Collection</span>\n",
    "\n",
    "### Materials\n",
    "- Microphone front-end circuit\n",
    "- Arduino + USB\n",
    "\n",
    "We will begin by collecting our data. To do so, choose 6 easily distinguishable words (or sounds) to ensure better classification accuracy.\n",
    "\n",
    "When humans distinguish words, they listen for temporal and frequency differences to determine what is being said. However, S1XT33N will have to choose much simpler features for classification: syllables, intonation, and magnitude. Keep this in mind when choosing your words.\n",
    "\n",
    "When you think of speech signals, you might notice that each word/sound has a disctinctive shape. Taking the shape of the magnitude of a signal is called enveloping, exemplified in the plot below. We want to do some filtering to retrieve the envelope of the audio signal, which we will use for calculating the SVD.\n",
    "\n",
    "<center>\n",
    "<img width=\"400px\" src=\"images/proj-envelope.png\">\n",
    "</center>\n",
    "\n",
    "**Given this information, brainstorm a list of 6 words/sounds to use**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Now we will record 40-45 audio samples for each of your 6 words. We recommend that you split the words among your lab group to maximize variance between the recorded signals. For each word, make sure to note who said it and how it was said (like through a video/audio recording on your phone) so that it is easier to reproduce later for live classification. **Note: Each word should only be recorded by one person. But partners should split between different word recordings.**\n",
    "\n",
    "Please read the full instructions for these tasks before proceeding.\n",
    "**For each chosen word, do the following:**\n",
    "1. Upload the sketch **`collect-data-envelope.ino`** to your Arduino.\n",
    "    - This sketch records $\\approx 2$ seconds of audio sampled every 0.35ms and sends the raw data to your computer.\n",
    "2. Read in Arduino data using your computer\n",
    "    - Download and open the `collect-data-envelope` folder to your local computer. Press `Shift + right click` within the file explorer and click \"Git Bash here\" to open a terminal window in the directory.\n",
    "    - Make sure the Serial Monitor/Plotter is closed\n",
    "    - Run **`python collect-data-envelope.py YOUR_WORD.csv`**.\n",
    "        - This program will capture audio data collected by the Arduino and append it to `YOUR_WORD.csv`. You should see a console output in your Git bash terminal after each sample is recorded. \n",
    "3. **When the Arduino light(s) are on, say the word you want to record. The Arduino is recording only when the light(s) on, so finish the word before the LEDs turns off.**\n",
    "    - The three LEDs on the Arduino will progressively turn on to help you gauge how much time you have.\n",
    "    - **Pronounce the word consistently**\n",
    "    - \"Good\" audio data has a high signal to noise ratio (SNR). Recording words while far away from the microphone may cause your intended word to blend in with background noise. However, speaking too loudly and/or too closely into the mic) may also cause your output to rail. You can probe the microphone output using the oscilloscope to check your output.\n",
    "    - **We recommend that after taking 3 or 4 recordings for the first time, stop the program (e.g. by pressing Ctrl + C in the command prompt) and check `YOUR_WORD.csv.` make sure that it looks like a sound wave as opposed to being full of super low values. *It might help to graph the data as a line plot in Excel.*** You don't want to record 45 times only to find that your mic board wasn't working.\n",
    "4. Once you've recorded 45 audio samples of the word, the Python program will automatically stop. You can terminate earlier if you would like by pressing Ctrl + C.\n",
    "5. Go into the .csv file and delete outlier samples such that you are left with **exactly 40 audio recordings of the word**. Outliers are often near the beginning and end of the .csv file when you may not be speaking. The best way to help you identity outlier samples is to plot the data using a line plot in Excel (you can plot individual rows to identify specific recordings). **Don't worry about aligning the data,** we will account for this in Part 3!\n",
    "6. If you are working on the lab on DataHub, you will need to upload your `.csv` files to DataHub into the `PCA_data` folder so the Jupyter notebook can access them.\n",
    "\n",
    "\n",
    "### Before moving on, please note that:\n",
    "\n",
    "You may realize in the next section that one or two of your words are not sorting quite as well as you would like. Don't be afraid to come back to this section and try collecting different words based on what you have learned makes a word sortable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "## <span style=\"color:navy\">Part 3: Data Preprocessing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different recordings of the same word can look wildly different, depending on factors like when you started saying the word and how quickly you said it (assuming you are not a robot that can repeat the word 50 times exactly the same way). Thus, before we can use the recorded data for PCA, we must first process the data. \n",
    "\n",
    "Using the Arduino, we have already implemented the first step of processing the audio recording: enveloping. It is not necessary for you to understand the enveloping function well enough to implement it (since we have already done it for you), but just in case you are curious, the enveloping function is described in the following pseudocode:\n",
    "\n",
    "<code><b>Enveloping function</b>\\\n",
    "Divide the whole signal to a block of 16 samples\\\n",
    "For each chunk:\\\n",
    "    Find the mean of the chunk\\\n",
    "    Subtract each sample by the mean\\\n",
    "    Find the sum of the absolute value of each sample\n",
    "</code>\n",
    "\n",
    "### 3.1 Load Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import csv\n",
    "import utils\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "cm = ['blue', 'red', 'green', 'orange', 'black', 'purple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Fill in the six words you recorded\n",
    "all_words_arr = ['', '', '', '', '', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by splitting our data into a training and testing set with a 70/30 split. Run the code below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv\n",
    "train_test_split_ratio = 0.7\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "# Build the dictionary of train and test samples.\n",
    "for i in range(len(all_words_arr)):\n",
    "    word_raw = utils.read_csv(\"PCA_data/{}.csv\".format(all_words_arr[i]))\n",
    "    word_raw_train, word_raw_test = utils.train_test_split(word_raw, train_test_split_ratio)\n",
    "    train_dict[all_words_arr[i]] = word_raw_train\n",
    "    test_dict[all_words_arr[i]] = word_raw_test\n",
    "\n",
    "# Count the minimum number of samples you took across the six recorded words. These variables might be useful for you!\n",
    "num_samples_train = min(list(map(lambda x : np.shape(x)[0], train_dict.values())))\n",
    "num_samples_test = min(list(map(lambda x : np.shape(x)[0], test_dict.values())))\n",
    "\n",
    "# Crop the number of samples for each word to the minimum number so all words have the same number of samples.\n",
    "for key, raw_word in train_dict.items():\n",
    "    train_dict[key] = raw_word[:num_samples_train,:]\n",
    "\n",
    "for key, raw_word in test_dict.items():\n",
    "    test_dict[key] = raw_word[:num_samples_test,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot your training data and visualize the enveloped version.\n",
    "\n",
    "**<span style=\"color:red\">Important: It's okay if the recordings aren't aligned. The code in the next part will align the data.</span>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all training samples\n",
    "word_number = 0\n",
    "selected_words_arr = all_words_arr\n",
    "for word_raw_train in train_dict.values():\n",
    "    plt.plot(word_raw_train.T)\n",
    "    plt.title('Training sample for \"{}\"'.format(selected_words_arr[word_number]))\n",
    "    word_number += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Align Audio Recordings\n",
    "\n",
    "As seen above, the speech is a fraction of the 2 second window, and each sample starts at different times. PCA is not good at interpreting delay, so we need to align the recordings and trim to a smaller segment of the sample where the speech is present. To do this, we will use a thresholding algorithm.\n",
    "\n",
    "First, we define a **`threshold`** relative to the maximum value of the data. We say that any signal that crosses the threshold is the start of a speech command. In order to not lose the first couple samples of the speech command, we say that the command starts **`pre_length`** samples _before_ the threshold is crossed. We then use a window of the data that is **`length`** long, and try to capture the entire command in that window.\n",
    "\n",
    "<b>The parameters `length`, `pre_length` and `threshold`</b> are defined with default values in the cells below. The default length value corresponds to a reasonable length for vectors we can use, because the Arduino has limited storage and memory and cannot store the entire sample.\n",
    "\n",
    "**Play around with the parameters `length`, `pre_length` and `threshold`** in the cells below to find appropriate values corresponding to your voice and chosen commands. You should see the results and how much of your command you captured in the plots generated below. When you are satisfied, note down the values of length, pre_length and threshold - you will need to add them to the Arduino sketch later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippets(data, length, pre_length, threshold):\n",
    "    \"\"\"Attempts to align audio samples in data.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Matrix where each row corresponds to a recording's audio samples.\n",
    "        length (int): The length of each aligned audio snippet.\n",
    "        pre_length (int): The number of samples to include before the threshold is first crossed.\n",
    "        threshold (float): Used to find the start of the speech command. The speech command begins where the\n",
    "            magnitude of the audio sample is greater than (threshold * max(samples)).\n",
    "    \n",
    "    Returns:\n",
    "        Matrix of aligned recordings.\n",
    "    \"\"\"\n",
    "    assert isinstance(data, np.ndarray) and len(data.shape) == 2, \"'data' must be a 2D matrix\"\n",
    "    assert isinstance(length, int) and length > 0, \"'length' of snippet must be an integer greater than 0\"\n",
    "    assert 0 <= threshold <= 1, \"'threshold' must be between 0 and 1\"\n",
    "    snippets = []\n",
    "\n",
    "    # Iterate over the rows in data\n",
    "    for recording in data:\n",
    "        # Find the threshold\n",
    "        recording_threshold = threshold * np.max(recording)\n",
    "\n",
    "        # Figure out when interesting snippet starts\n",
    "        i = pre_length\n",
    "        while recording[i] < recording_threshold:\n",
    "            i += 1\n",
    "            \n",
    "        snippet_start = min(i - pre_length, len(recording) - length)\n",
    "        snippet = recording[snippet_start:snippet_start + length]\n",
    "\n",
    "        # Normalization\n",
    "        snippet = snippet / np.sum(snippet)\n",
    "        \n",
    "        snippets.append(snippet)\n",
    "\n",
    "    return np.vstack(snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for get_snippets\n",
    "def process_data(dict_raw, length, pre_length, threshold, plot=True):\n",
    "    \"\"\"\n",
    "    Process the raw data given parameters and return it.\n",
    "    \n",
    "    Args:\n",
    "        dict_raw (np.ndarray): Raw data collected.\n",
    "        data (np.ndarray): Matrix where each row corresponds to a recording's audio samples.\n",
    "        length (int): The length of each aligned audio snippet.\n",
    "        pre_length (int): The number of samples to include before the threshold is first crossed.\n",
    "        threshold (float): Used to find the start of the speech command. The speech command begins where the\n",
    "            magnitude of the audio sample is greater than (threshold * max(samples)).\n",
    "        plot (boolean): Plot the dataset if true.\n",
    "            \n",
    "    Returns:\n",
    "        Processed data dictionary.\n",
    "    \"\"\"\n",
    "    processed_dict = {}\n",
    "    word_number = 0\n",
    "    for key, word_raw in dict_raw.items():\n",
    "        word_processed = get_snippets(word_raw, length, pre_length, threshold)\n",
    "        processed_dict[key] = word_processed\n",
    "        if plot:\n",
    "            plt.plot(word_processed.T)\n",
    "            plt.title('Samples for \"{}\"'.format(selected_words_arr[word_number]))\n",
    "            word_number += 1\n",
    "            plt.show()\n",
    "            \n",
    "    return processed_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 80 # Default: 80\n",
    "pre_length = 5 # Default: 5\n",
    "threshold = 0.5 # Default: 0.5\n",
    "\n",
    "processed_train_dict = process_data(train_dict, length, pre_length, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now notice the improved alignment for the samples. Can you tell which word is which just by the envelope? If any of your words look too similar to each another, then PCA will likely have a difficult time distinguishing between them, so you may want to consider re-choosing/recording more unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4'></a>\n",
    "## <span style=\"color:navy\">Part 4: PCA via SVD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 SVD/PCA Resources\n",
    "- http://www.ams.org/publicoutreach/feature-column/fcarc-svd\n",
    "- https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
    "- https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate and Preprocess PCA Matrix\n",
    "\n",
    "Now that we have our aligned data, we will build the PCA input matrix from that data by **stacking all the data vertically**.\n",
    "\n",
    "**Sanity check:** What should be the dimensions of `processed_A`? Feel free to use `np.shape()` if you aren't sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_A = np.vstack(list(processed_train_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of PCA is to center the data's mean at zero and store it in `demeaned_A`. Please note that you want to **get the mean of each feature** (***what are the features?***). The function [`np.mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) might be helpful here, along with specifying the `axis` parameter.\n",
    "\n",
    "<!-- which can be helpful to obtain principal components that are representative of the shape of the variations in the data. -->\n",
    "\n",
    "**Sanity check:** Does the shape of `mean_vec` make sense given what we averaged across?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-mean the matrix A\n",
    "# YOUR CODE HERE\n",
    "mean_vec = ...\n",
    "demeaned_A = ...\n",
    "print(processed_A.shape)\n",
    "print(mean_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Principal Component Analysis\n",
    "\n",
    "Next, take the SVD of your demeaned data - `np.linalg.svd` may be useful here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the SVD of matrix demeaned_A \n",
    "# YOUR CODE HERE #\n",
    "U, S, Vt = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect your sigma values. They should tell you how many principal components you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out the sigma values (Hint: Use plt.stem for a stem plot)\n",
    "# YOUR CODE HERE #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">How many principal components do you need? Given that you are sorting 6 words, what is the number you expect to need?</span>** \n",
    "\n",
    "There is no correct answer here. We can pick as many principal components as we want, but we will experience diminshing returns may start capturing noise. In our project, since we are loading these basis vectors onto the Arduino Leonardo, we can only store at most 3 principal components before we run into memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Choosing a Basis using Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `new_basis` argument to be a basis of the first 3 principal components. (Hint: Of the three outputs from the SVD function call, which one will contain the principal components onto which we want to project our data points? Do we need to transpose it? **The lab note will help!**)\n",
    "\n",
    "When you plot `new_basis` you should see a number of line plots equal to the number of principal components you've chosen (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the principal component(s)\n",
    "# YOUR CODE HERE\n",
    "new_basis = ...\n",
    "plt.plot(new_basis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now project the data in the matrix A onto the new basis and plot it. For three principal components, in addition to the 3D plot, we also provided 2D plots which correspond to the top and side views of the 3D plot. Do you see clustering? Do you think you can separate the data easily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Project the data onto the new basis\n",
    "# YOUR CODE HERE. Hint: np.dot() may help, as well as printing the dimensions.\n",
    "proj = ...\n",
    "\n",
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(all_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_train:num_samples_train*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_train:num_samples_train*(i+1),1], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], edgecolor='none')\n",
    "\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in many AI applications, the data above are noisy, so we expect some classification errors. The important part is that you see strong clustering of your words. \n",
    "\n",
    "If you don't see clustering, try to think about why this might be the case. Things you might want to ask yourself:\n",
    "- How does PCA create the clusters? \n",
    "- Which characteristics of your waveform will PCA favor when clustering? \n",
    "- How can you choose your words to maximize the differences between the classes?\n",
    "\n",
    "Once you think you have decent clustering, move on to automating classification. **Choose 4 out of the 6 words which form the most distinct clusters. You will be using these four words for the rest of this lab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "selected_words_arr = ['', '', '', '']\n",
    "\n",
    "# Select data\n",
    "selected_train_dict = {k: train_dict[k] for k in selected_words_arr}\n",
    "selected_processed_train_dict = {k: processed_train_dict[k] for k in selected_words_arr}\n",
    "selected_test_dict = {k: test_dict[k] for k in selected_words_arr}\n",
    "\n",
    "num_samples_train = min(list(map(lambda x : np.shape(x)[0], selected_train_dict.values())))\n",
    "num_samples_test = min(list(map(lambda x : np.shape(x)[0], selected_test_dict.values())))\n",
    "\n",
    "# Reconstruct and demean data based on 4 chosen words.\n",
    "# Zero-mean the matrix new A\n",
    "processed_A = np.vstack(list(selected_processed_train_dict.values()))\n",
    "mean_vec = ...\n",
    "demeaned_A = ...\n",
    "print(processed_A.shape)\n",
    "print(mean_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same process as earlier, recalculate the new basis using only your four chosen words so that the basis best represents the four chosen words instead of all of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the SVD of matrix demeaned_A (np.linalg.svd)\n",
    "# YOUR CODE HERE #\n",
    "U, S, Vt = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out the sigma values (Hint: Use plt.stem for a stem plot)\n",
    "# YOUR CODE HERE #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the principal component(s)\n",
    "# YOUR CODE HERE\n",
    "new_basis = ...\n",
    "plt.plot(new_basis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell projects the demeaned input matrix onto the new basis to compress the data for more practical classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = demeaned_A.dot(new_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part5'></a>\n",
    "## <span style=\"color:navy\">Part 5: Clustering Data Points</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement `find_centroids`, which finds the center of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centroids(clustered_data, num_samples_train):\n",
    "    \"\"\"Find the center of each cluster by taking the mean of all points in a cluster.\n",
    "    It may be helpful to recall how you constructed the data matrix (e.g. which rows correspond to which word)\n",
    "    \n",
    "    Parameters:\n",
    "        clustered_data: the data already projected onto the new basis\n",
    "        num_samples_train: the number of samples trained\n",
    "        \n",
    "    Returns: \n",
    "        The centroids of the clusters\n",
    "    \"\"\"\n",
    "    centroids = []\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: the variable num_samples_train may help you splice into your clustered_data, as well as np.mean() with the axis option\n",
    "    # Feel free to ignore the skeleton code if you wish to write it your way.\n",
    "    \n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code below to find the centroids of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the centroids of each cluster\n",
    "# YOUR CODE HERE: hint: call find_centroids()\n",
    "centroids = ...\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to plot your centroids along with your projected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_list = np.vstack(centroids)\n",
    "colors = cm[:(len(centroids))]\n",
    "\n",
    "for i, centroid in enumerate(centroid_list):\n",
    "    print('Centroid {} is at: {}'.format(i, str(centroid)))\n",
    "\n",
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_train:num_samples_train*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(selected_words_arr, loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        Axes3D.scatter(ax, *np.array([centroids[i]]).T, c=cm[i], marker = '*', s=300)\n",
    "    plt.title(\"Training Data\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_train:num_samples_train*(i+1),1], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(selected_words_arr, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axs[0].scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    axs[1].scatter(centroid_list[:,0], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    axs[2].scatter(centroid_list[:,1], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    plt.show()\n",
    "\n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=colors[i], edgecolor='none')\n",
    "\n",
    "    plt.scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    plt.legend(selected_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(\"Training Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part6'></a>\n",
    "## <span style=\"color:navy\">Part 6: Testing your Classifier</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have the means (centroid) for each word, let's evaluate performance. Recall that we will classify each data point according to the centroid with the least Euclidian distance to it.\n",
    "\n",
    "Before we perform classification, we need to do the same preprocessing to the test data that we did to the training data (enveloping, demeaning, projecting onto the PCA basis). You have already written most of the code for this part. However, note the difference in variable names as we are now working with test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at what our raw test data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all test samples\n",
    "word_number = 0\n",
    "for word_raw_test in selected_test_dict.values():\n",
    "    plt.plot(word_raw_test.T)\n",
    "    plt.title('Test sample for \"{}\"'.format(selected_words_arr[word_number]))\n",
    "    word_number += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform enveloping and trimming of our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_dict = process_data(selected_test_dict, length, pre_length, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the PCA matrix by stacking all the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_processed_test_dict = {k: processed_test_dict[k] for k in selected_words_arr}\n",
    "\n",
    "processed_A_test = np.vstack(list(selected_processed_test_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will do something slightly different.**\n",
    "\n",
    "Previously, you projected data onto your PCA basis with $ (x - \\bar{x})P $, where $\\bar{x}$ is the mean vector, $x$ is a single row of `processed_A`, and $P$ is `new_basis`. \n",
    "\n",
    "We can rewrite this operation as:\n",
    "\n",
    "$$ (x - \\bar{x})P = xP - \\bar{x}P = xP - \\bar{x}_{\\text{proj}} $$ \n",
    "$$ \\bar{x}_{\\text{proj}} = \\bar{x}P $$\n",
    "\n",
    "Why might we want to do this? We'll later perform these operations on our car. Our Arduinos have limited memory, so we want to store as little as possible. Instead of storing a length $n$ vector $\\bar{x}$, we can precompute $ \\bar{x}_{\\text{proj}} \\in \\mathbb{R}^3$ and store that instead!\n",
    "\n",
    "Compute $ \\bar{x}_{\\text{proj}} $ using the **same mean vector** as the one computed with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "projected_mean_vec = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the test data onto the **same PCA basis** as the one computed with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "projected_A_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-mean the projected test data using the **`projected_mean_vec`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "proj = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the projections to see how well your test data clusters in this new basis. This will give you an idea of test classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_test:num_samples_test*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(selected_words_arr,loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    plt.title(\"Test Data\")\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        Axes3D.scatter(ax, *np.array([centroids[i]]).T, c=cm[i], marker = '*', s=300)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_test:num_samples_test*(i+1),1], proj[i*num_samples_test:num_samples_test*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(selected_words_arr, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axs[0].scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    axs[1].scatter(centroid_list[:,0], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    axs[2].scatter(centroid_list[:,1], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    fig.show()\n",
    "\n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),1], c=colors[i], edgecolor='none')\n",
    "\n",
    "    plt.scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    plt.legend(selected_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(\"Test Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some idea of how our test data looks in our PCA basis, let's see how our data actually performs. Implement the `classify` function which takes in a data point after enveloping is applied and returns which word number it belongs to depending on the closed centroid in Euclidian distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data_point, new_basis, projected_mean_vec, centroids):\n",
    "    \"\"\"Classifies a new voice recording into a word.\n",
    "    \n",
    "    Args:\n",
    "        data_point: new data point vector before demeaning and projection\n",
    "        new_basis: the new processed basis to project on\n",
    "        projected_mean_vec: the same projected_mean_vec as before\n",
    "    Returns:\n",
    "        Word number (should be in {1, 2, 3, 4} -> you might need to offset your indexing!)\n",
    "    Hint:\n",
    "        Remember to use 'projected_mean_vec'!\n",
    "        np.argmin(), and np.linalg.norm() may also help!\n",
    "    \"\"\"\n",
    "    # TODO: classify the demeaned data point by comparing its distance to the centroids\n",
    "    # YOUR CODE HERE\n",
    "    projected_data_point = ...\n",
    "    demeaned = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out the classification function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the classification function\n",
    "print(classify(processed_A_test[0,:], new_basis, projected_mean_vec, centroids)) # Modify the row index of processed_A_test to use other vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our goal is 80% accuracy for each word.** Apply the `classify` function to each sample and compute the accuracy for each word. If you do not meet 80% accuracy for each word, try to find different combinations of words/parameters that result in more distinct clusters in the plots of the projected data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to classify the whole A matrix\n",
    "correct_counts = np.zeros(4)\n",
    "\n",
    "for (row_num, data) in enumerate(processed_A_test):\n",
    "    word_num = row_num // num_samples_test + 1\n",
    "    if classify(data, new_basis, projected_mean_vec, centroids) == word_num:\n",
    "        correct_counts[word_num - 1] += 1\n",
    "        \n",
    "for i in range(len(correct_counts)):\n",
    "    print(\"Percent correct of word {} = {}%\".format(i + 1, 100 * correct_counts[i] / num_samples_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part7'></a>\n",
    "## <span style=\"color:navy\">Part 7: Arduino Implementation of PCA Classify</span>\n",
    "\n",
    "### Materials\n",
    "- Microphone front-end circuit\n",
    "- Arduino + USB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our completed classifier, we are now ready to classify live commands using our Arduino. This section will walk you through implementing your classification algorithm on the Arduino. You will need to transfer the preprocessing parameters, PCA vectors, projected mean vector, and centroids all into the Arduino. **You will be copying and pasting the code from the Appendix of the SVD/PCA notebook into `classify.ino.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task will be to implement your <b>data processing</b> and <b>classification</b> (just the projection, not the PCA) in the Arduino sketch <b>`classify.ino`</b>. Since Arduino does not have as many built-in functions as Python, you will have to write out the functions yourself. For example, a dot product should be written as:\n",
    "\n",
    "```C\n",
    "float dot_product_result = 0;\n",
    "for (i = 0; i < LENGTH; i++) {\n",
    "    dot_product_result += vector1[i] * vector2[i];\n",
    "}\n",
    "```\n",
    "where `dot_product_result` is the result of the dot product, and `vector1` and `vector2` are the two vectors you're taking the dot product of.\n",
    "\n",
    "NOTE: The coding language is a derivative of C/C++, so you need to follow C syntax! i.e. declaring variables before using them, using `{` and `}` to denote the start and end of a for loop, adding a `;` to the ends of lines, etc. You can reference the existing code for examples of how to use the proper syntax.\n",
    "\n",
    "For debugging purposes, you can add print statements to the code. Printing to the Arduino IDE's Serial Monitor looks like the line below.\n",
    "\n",
    "`Serial.println(\"I'm being printed!\");`\n",
    "\n",
    "There are 3 code blocks (`PCA1/2/3`) that you need to modify. <b>You should not have to change anything else outside these marked code blocks and the pin definition if you're using a different pin than the default `A2` pin.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`CODE BLOCK PCA1`**\n",
    "- Copy the `SNIPPET_SIZE`, `PRELENGTH` and `THRESHOLD` from the appendix section of this notebook.\n",
    "- Set `BASIS_DIM` to be the number of principal components.\n",
    "- Read the following to set `EUCLIDEAN_THRESHOLD` and `LOUDNESS_THRESHOLD`.\n",
    "\n",
    "**`EUCLIDEAN_THRESHOLD` filters the classification depending on a sample's distance to the closest centroid.** Look at the plot of your data clusters and the centroids from earlier in this notebook and approximate a radius around the centroids that capture most of the data. **Try to be conservative - it's better to not classify than to misclassify.**\n",
    "\n",
    "**`LOUDNESS_THRESHOLD` filters the classification depending on the amplitude of the recorded data.** Look at the plots of the audio recordings to determine the loudness of your recordings, then determine an appropriate threshold which isn't low enough to be considered noise. If the Arduino classifies noise, increase this constant. If it misses a lot of speech (i.e. thinks your word is noise), decrease this constant. This variable is used internally in the enveloping function. **Note: this value should be on the scale of 10s to 100s** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`CODE BLOCK PCA2`**\n",
    "- Copy the PCA vectors, projected mean vector, and centroids from the appendix section of the SVD/PCA ipynb.\n",
    "- If you are using 3 principal components, add a new `pca_vec3` array and make sure `BASIS_DIM` in `CODE BLOCK PCA1` is 3 (default value is 2).\n",
    "- Note: Using more principal components increases the dimensionality of the centroids and projections, but also consumes more of the limited memory on the Arduino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`CODE BLOCK PCA3`**\n",
    "- This is the actual classification algorithm\n",
    "- Before this block, the call to `envelope_small` leaves the data vector of your recording in the array called `result`.\n",
    "- Project this data onto your new PCA basis.\n",
    "    - **Since the principal component vectors are orthonormal, they have unit norm. Thus when you perform the projection of the recorded data vector onto the principal components, you only need to calculate the dot products between those vectors.**\n",
    "    - Use just one loop to project your recorded data vector onto both (or all 3, if you're using 3) of your PCA vectors.\n",
    "    - Use the variables `proj1` and `proj2` to store the projection results. If you're using 3 vectors, create a variable `proj3` in the same way `proj1` and `proj2` are defined.\n",
    "- Demean the projection.\n",
    "    - Remember that we **demean after projecting** to save memory on the Arduino. Instead of finding $x - \\bar{x}$ and then projecting onto the PCA basis, we first project and then demean, using $y = x_{\\text{proj}} - \\bar{x}_{\\text{proj}} $, where $x_{\\text{proj}}$ is the projection of your data vector onto the PCA basis, and $\\bar{x}_{\\text{proj}}$ is the projection of your mean vector onto the PCA basis.\n",
    "- Classify the projections using the centroids\n",
    "    - Find the distance between the projected data point and each centroid using the function `l2_norm` (for 2 principal components) or `l2_norm3` (for 3 principal components). Look up the function definition in the sketch.\n",
    "    - Out of the 4 centroids, find the one with the smallest L2 norm.\n",
    "    - Verify this distance is less than `EUCLIDEAN_THRESHOLD`. If it's not, print out an error statement saying this threshold was not satisfied so you have an easier time debugging.\n",
    "- Print the classification to the Serial Monitor. The baud rate for this program is 38400, so make sure you change the baud rate of your Serial Monitor accordingly!\n",
    "\n",
    "Before testing the code, probe the mic board's output with your oscilloscope and make sure that it is still centered around 2.5V. Now upload the sketch, (re)open the Serial Monitor, and press the reset button. Say your word and the Arduino should recognize it! Try to get a reasonable accuracy (like at least 80-90%); it's okay if it misclassifies occasionally, but it should be accurate in general!\n",
    "\n",
    "**If the Arduino does not classify as well as you think it should, remember to play with the `EUCLIDEAN_THRESHOLD` and `LOUDNESS_THRESHOLD` variables.** To debug the sketch, you can also print out any of the variables you have used, like the distance to the closest centroid.\n",
    "\n",
    "Voila! Your S1XT33N car can recognize your words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Summary TODO</span>**\n",
    "- **<span style=\"color:red\">Fill in `CODE BLOCK PCA1`: Fill out `SNIPPET_SIZE`, `PRELENGTH`, `THRESHOLD`, `EUCLIDEAN_THRESHOLD`, and `LOUDNESS_THRESHOLD`</span>** \n",
    "- **<span style=\"color:red\">Fill in `CODE BLOCK PCA2`: Copy the principal components, projected mean vector, and centroids from the SVD/PCA Jupyter notebook</span>**\n",
    "- **<span style=\"color:red\">Fill in `CODE BLOCK PCA3`: Do the actual classification.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width='30px' align='left' src=\"http://inst.eecs.berkeley.edu/~ee16b/sp16/lab_pics/check.png\">\n",
    "\n",
    "## <span style=\"color:green\">CHECKOFF</span>\n",
    "\n",
    "### Checkoff Form (https://eecs16b.org/lab-checkoff)\n",
    "\n",
    "\n",
    "- **Have all questions, code, and plots completed in this notebook.** Your GSI will check all your PCA code and plots.\n",
    "- **Show your GSI that you've achieved 80% accuracy on your test data for all 4 words.** \n",
    "- **Show your GSI that you are able to classify live while running `classify.ino`.**\n",
    "- **Be prepared to answer conceptual questions about the lab.** Make sure you have read the lab note before requesting a checkoff! Many checkoff questions are pulled straight from the lab note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE ALL YOUR DATA!!\n",
    "\n",
    "- Make sure to save the formatted vectors below and `classify.ino` for Integration/Final Demo!\n",
    "- **Data stored on the lab computers often gets deleted automatically.** Please store it on your personal flash drive or cloud storage like Google Drive, and not on the lab computers! If you used DataHub, it should save through your CalNet ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='appendix'></a>\n",
    "## <span style=\"color:navy\">Appendix: Formatting Vectors for Arduino</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code blocks and copy/paste the following printed code into **`classify.ino`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Paste the code below into 'CODE BLOCK PCA1':\")\n",
    "print(\"\")\n",
    "print(utils.format_constant_c(\"SNIPPET_SIZE\", length))\n",
    "print(utils.format_constant_c(\"PRELENGTH\", pre_length))\n",
    "print(utils.format_constant_c(\"THRESHOLD\", threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Paste the code below into 'CODE BLOCK PCA2':\")\n",
    "print(\"\")\n",
    "print(utils.format_array_c(\"pca_vec1\", new_basis[:, 0]))\n",
    "print(utils.format_array_c(\"pca_vec2\", new_basis[:, 1]))\n",
    "if new_basis.shape[1] == 3:\n",
    "    print(utils.format_array_c(\"pca_vec3\", new_basis[:, 2]))\n",
    "print(utils.format_array_c(\"projected_mean_vec\", projected_mean_vec))\n",
    "print(utils.format_array_c(\"centroid1\", centroids[0]))\n",
    "print(utils.format_array_c(\"centroid2\", centroids[1]))\n",
    "print(utils.format_array_c(\"centroid3\", centroids[2]))\n",
    "print(utils.format_array_c(\"centroid4\", centroids[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part8'></a>\n",
    "# <span style=\"color:navy\">Part 8: Final Lab Report Questions</span>\n",
    "-----\n",
    "\n",
    "The final lab report tests your understanding of EECS 16B Labs 6-9, with an emphasis on conceptual and\n",
    "analytical understanding. It also allows you to look at these labs from a bigger picture and reflect on your design\n",
    "process and choices. The entire final lab report will be due on **Friday, 5/03**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- Give a summary in your own words of what you did in this lab.\n",
    "\n",
    "**1. What 4 words did you choose for classification? What characteristics of this set make your words good for classification? Provide at least two features. Compared to other sets of words with similar ideally good characteristics, why is this set preferable to others?**\n",
    " \n",
    "**2. Why is taking the envelope your the voice signals a good choice for classification, especially considering that this classifier is implemented on an Arduino?**\n",
    "\n",
    "**3. Why do we need to use SVD/PCA to represent our data set?**\n",
    "\n",
    "**4. If we were to use the transpose of our data matrix for SVD, which rows/columns of which matrix correspond to the principal component vectors representing the recorded words? Why?**\n",
    "\n",
    "**5. How many basis vectors are you using, and how did you choose this number? What are the benefits vs. tradeoffs of increasing or decreasing the number of basis vectors by a small amount? What about for increasing the number of basis vectors by a large amount?**\n",
    "\n",
    "**6. What are length, prelength, and threshold for our data pre-processing? How does changing them affect your alignment? Include both the definitions and the values you chose. What kinds of words are better suited for our pre-processing method with live classification?**\n",
    "\n",
    "**7. Why can we simply take the dot product when projecting our recorded data vector onto the principal component vectors?**\n",
    "\n",
    "**8. What is `EUCLIDEAN_THRESHOLD`? What is `LOUDNESS_THRESHOLD`? Include both the definitions and the values you chose. During live classification, which threshold was more difficult to satisfy, and how do you know?**\n",
    "\n",
    "\n",
    "**9. How different was live classification in practice from what you found in the SVD/PCA lab? Why do you think that is?**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
